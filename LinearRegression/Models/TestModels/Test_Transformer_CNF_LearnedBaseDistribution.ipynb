{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PFNExperiments.LinearRegression.Models.Transformer import MLP\n",
    "from PFNExperiments.LinearRegression.Models.ModelPosterior import ModelPosteriorFullGaussian2\n",
    "from PFNExperiments.LinearRegression.Models.Transformer_CNF_DoubleCondition2 import TransformerDecoderConditionalDouble_parallel\n",
    "from PFNExperiments.LinearRegression.Models.Transformer_CNF_LearnedBaseDistribution import TransformerReturnRepresentations, TransformerCNFConditionalDecoderDouble_parallel_learnedBaseDistribution\n",
    "from PFNExperiments.Training.FlowMatching.CFMLossOTGaussianBase import CFMLossOTGaussianBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = 10\n",
    "N = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arik_\\Documents\\Dokumente\\Job_Clausthal\\PFNs\\Repository\\PFNExperiments\\.conda\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the pre-trained encoder model\n",
    "\n",
    "encoder = TransformerReturnRepresentations(\n",
    "    n_features= P+1,\n",
    "    seq_len= N,\n",
    "    d_model= 512,\n",
    "    n_heads= 8,\n",
    "    dim_feedforward=512*2,\n",
    "    dropout_rate = 0.1,\n",
    "    n_layers=6,\n",
    "    n_skip_layers_final_heads = 2,\n",
    "    n_output_units_per_head=[P, P*P, P]\n",
    ")\n",
    "\n",
    "encoder.load_state_dict(torch.load(\"C:/Users/arik_/Documents/Dokumente/Job_Clausthal/PFNs/Repository/PFNExperiments/LinearRegression/Models/TestModels/SavedBaseModel/model.pth\", map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the correct class to transform model predictions to posterior samples\n",
    "model_posterior = ModelPosteriorFullGaussian2(cov_reg_factor = 1e-3, use_lowrank_normal = False, diag_transform = lambda x: x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_to_process_encoder_output = MLP(\n",
    "    n_input_units = encoder.d_model,\n",
    "    n_output_units= 1024,\n",
    "    n_hidden_units= 1024,\n",
    "    n_skip_layers= 3,\n",
    "    dropout_rate = 0.1\n",
    ")\n",
    "\n",
    "mlp_to_process_time_conditioning = MLP(\n",
    "    n_input_units = 1,\n",
    "    n_output_units = 64,\n",
    "    n_hidden_units= 64,\n",
    "    n_skip_layers= 2,\n",
    "    dropout_rate = 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = TransformerDecoderConditionalDouble_parallel(\n",
    "    n_input_features= P,\n",
    "    d_model_decoder= 256,\n",
    "    d_model_encoder= encoder.d_model,\n",
    "    n_heads= 8,\n",
    "    d_ff = 256*2,\n",
    "    dropout= 0.1,\n",
    "    n_condition_features_a = mlp_to_process_encoder_output.n_output_units,\n",
    "    n_condition_features_b = mlp_to_process_time_conditioning.n_output_units,\n",
    "    n_layers= 1,\n",
    "    use_positional_encoding=False,\n",
    "    use_self_attention=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_to_process_decoder_output = MLP(\n",
    "    n_input_units = decoder.d_model_decoder,\n",
    "    n_output_units= 512,\n",
    "    n_hidden_units= P,\n",
    "    n_skip_layers= 3,\n",
    "    dropout_rate = 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerCNFConditionalDecoderDouble_parallel_learnedBaseDistribution(\n",
    "    encoder = encoder,\n",
    "    model_posterior = model_posterior,\n",
    "    mlp_to_process_encoder_output = mlp_to_process_encoder_output,\n",
    "    mlp_to_process_time_conditioning = mlp_to_process_time_conditioning,\n",
    "    mlp_to_process_decoder_output = mlp_to_process_decoder_output,\n",
    "    decoder = decoder,\n",
    "    freeze_encoder=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fun = CFMLossOTGaussianBase(\n",
    "    sigma_min=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4 \n",
    "test_x = torch.randn(BATCH_SIZE, N, P+1)\n",
    "test_z = torch.randn(BATCH_SIZE, P)\n",
    "test_t = torch.randn(BATCH_SIZE, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#z_t.shape, encoder_representation.shape, test_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 10])\n",
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "encoder_prediction, encoder_representation = model.forward_encoder(test_x)\n",
    "\n",
    "base_distribution_samples = model.get_base_distribution_samples(encoder_prediction)\n",
    "\n",
    "z_t, z_0_b = loss_fun.psi_t_conditional_fun(\n",
    "    z_0_a = base_distribution_samples,\n",
    "    z_1 = test_z,\n",
    "    t = test_t,\n",
    "    z_0_b = None,\n",
    ")\n",
    "\n",
    "\n",
    "vector_field_prediction = model.forward_decoder(\n",
    "    z = z_t,\n",
    "    x_encoder= encoder_representation,\n",
    "    condition_time= test_t\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtarget_vector_field\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
