[confic_created_on]
time = {'Mon Sep 16 08:33:31 2024'}

[name]
name = basic_config_lda_small_eval

[BASIC]
n = 25
p = 5
batch_size = 512
n_epochs = 150
n_samples_per_epoch = 100_000
n_samples_to_generate_at_once = 1_000
shuffle = False
save_path = /content/drive/MyDrive/PFN_Experiments/Training_RunsCFM/basic_config_lda_small_eval_Mon_Sep_16_08_33_31_2024
train_frac = 0.5
val_frac = 0.1
n_batches_per_epoch = 195

[DATA_GENERATION]
pprogram = lda_basic
scheduler_behaviour = All_constant
generate_x_behaviour = uniform
pprogram_params = {'n_docs': '50', 'n_words': '5', 'batch_size': '1024', 'n_topics': 3, 'alpha_dir': 0.1, 'beta_dir': 0.1, 'doc_len_max': 100, 'doc_len_mean': 10.0}

[MODEL]
type = TransformerCNFConditionalDecoderSequenceZ
n_input_features_encoder = 5
n_input_features_decoder = 5
d_model_encoder = 512
d_model_decoder = 512
n_heads_encoder = 8
n_heads_decoder = 8
d_ff_encoder = 512
d_ff_decoder = 512
dropout_encoder = 0.1
dropout_decoder = 0.1
n_conditional_input_features = 1
n_condition_features = 512
n_layers_condition_embedding = 3
n_layers_encoder = 8
n_layers_decoder = 6
use_positional_encoding_encoder = True
use_positional_encoding_decoder = True
use_self_attention_decoder = True
output_dim = 5


[TRAINING]
loss_function = CFMLossOT2
sigma_min = 0.0001
learning_rate = 1e-06
weight_decay = 1e-05
scheduler = OneCycleLR
scheduler_params = {'max_lr': 0.0005, 'epochs': '150', 'steps_per_epoch': 488, 'pct_start': 0.1, 'div_factor': 25.0, 'final_div_factor': 10000.0}
early_stopping_patience = 100000
max_grad_norm = 1.0

[EVALUATION]
n_samples_per_model = 1000
n_synthetic_cases = 5
real_world_eval = Basic1
n_evaluation_cases_real_world = All
do_full_evaluation = True
only_use_hmc_numpyro = True
save_path_data_real_world_eval = /
real_world_benchmark_id = 336
real_world_preprocessor = gmm_preprocessor_multivariate
results_dict_to_latent_variable_posterior_model = just_return_results_flatten_beta
results_dict_to_latent_variable_comparison_models = result_dict_to_latent_variable_convert_phi_to_beta_flatten
result_dict_to_data_for_comparison_models = results_dict_to_data_x_tuple
discrete_z = False

[FULL_MODEL]
sample_name = beta
sample_shape = (3, 5)
n_samples = 1000
batch_size = 256
solve_adjoint = True
atol = 1e-07
rtol = 1e-07

